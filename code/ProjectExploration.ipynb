{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"../data/glove/\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import string\n",
    "from scipy import spatial\n",
    "import sklearn as sk\n",
    "from sklearn import linear_model\n",
    "\n",
    "from glove import loadWordVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_dim = 200\n",
    "train_size = 323482\n",
    "test_size = 80870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../data/quora/quora_duplicate_questions.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ac52c5fa35c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/quora/quora_duplicate_questions.tsv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/quora/train.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/quora/test.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../data/quora/quora_duplicate_questions.tsv'"
     ]
    }
   ],
   "source": [
    "tokens = {}\n",
    "header = []\n",
    "count = 0\n",
    "with open('../data/quora/quora_duplicate_questions.tsv') as f, open('../data/quora/train.tsv', 'w') as g, open('../data/quora/test.tsv', 'w') as h:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    train_writer = csv.writer(g, delimiter='\\t')\n",
    "    test_writer = csv.writer(h, delimiter='\\t')\n",
    "    \n",
    "    index = 0\n",
    "    isHeader = True\n",
    "    for line in reader:\n",
    "        if isHeader:\n",
    "            header = line\n",
    "            isHeader = False\n",
    "            count += 1\n",
    "            continue\n",
    "            \n",
    "        if count <= train_size:\n",
    "            train_writer.writerow(line)\n",
    "        else:\n",
    "            test_writer.writerow(line)\n",
    "            \n",
    "        sent1 = line[3].translate(None, string.punctuation).lower().split()\n",
    "        sent2 = line[4].translate(None, string.punctuation).lower().split()\n",
    "        \n",
    "        if index <= 1:\n",
    "            print(line)\n",
    "            \n",
    "#         sent1 = str.translate(line[3], str.maketrans('', '', string.punctuation)).lower().split()\n",
    "#         sent2 = str.translate(line[4], str.maketrans('', '', string.punctuation)).lower().split()            \n",
    "        for word in sent1 + sent2:\n",
    "            if word not in tokens:\n",
    "                tokens[word] = index\n",
    "                index += 1\n",
    "        \n",
    "        count += 1\n",
    "            \n",
    "    tokens[\"UNK\"] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111682\n",
      "111682\n"
     ]
    }
   ],
   "source": [
    "word_vectors = loadWordVectors(tokens)\n",
    "print(len(tokens))\n",
    "print(len(word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.39396  ,  0.44185  , -0.0042279, ...,  0.47576  ,  0.20978  ,\n",
       "        -0.11687  ],\n",
       "       [ 0.32928  ,  0.25526  ,  0.26753  , ...,  0.074621 ,  0.012001 ,\n",
       "        -0.21952  ],\n",
       "       [-0.071549 ,  0.093459 ,  0.023738 , ...,  0.33617  ,  0.030591 ,\n",
       "         0.25577  ],\n",
       "       ..., \n",
       "       [ 0.91682  , -0.36737  , -0.32286  , ..., -0.3297   , -0.66926  ,\n",
       "        -0.75765  ],\n",
       "       [ 0.39356  ,  0.18569  ,  0.011526 , ...,  0.4215   ,  0.087896 ,\n",
       "         1.094    ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# zero_words = np.where(np.sum(word_vectors, axis=1) == 0)[0]\n",
    "# for i in range(len(zero_words)):\n",
    "#     print(word_vectors[i, :])\n",
    "# word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_to_vec(sentence, word_vectors):\n",
    "    vec_sum = np.zeros(num_dim)\n",
    "    count = .1\n",
    "    for word in sentence:\n",
    "        if word in tokens:\n",
    "            vec = word_vectors[tokens[word], :]\n",
    "            if np.sum(vec) != 0:\n",
    "                vec_sum += vec\n",
    "                count += 1\n",
    "                \n",
    "    if np.sum(vec_sum) == 0:\n",
    "        vec_sum = np.random.rand(num_dim)\n",
    "                \n",
    "    return vec_sum / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_distance(sentence1, sentence2, word_vectors):\n",
    "    vec1 = sentence_to_vec(sentence1, word_vectors)\n",
    "    vec2 = sentence_to_vec(sentence2, word_vectors)\n",
    "    \n",
    "    return spatial.distance.cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(sentence1, sentence2, word_vectors):\n",
    "    vec1 = sentence_to_vec(sentence1, word_vectors)\n",
    "    vec2 = sentence_to_vec(sentence2, word_vectors)\n",
    "    \n",
    "    return np.linalg.norm(vec1 - vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manhattan_distance(sentence1, sentence2, word_vectors):\n",
    "    vec1 = sentence_to_vec(sentence1, word_vectors)\n",
    "    vec2 = sentence_to_vec(sentence2, word_vectors)\n",
    "    \n",
    "    return np.sum(np.abs(vec1 - vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "X_train = np.zeros([train_size, 1])\n",
    "Y_train = np.zeros([train_size, 1])\n",
    "\n",
    "X_test = np.zeros([test_size, 1])\n",
    "Y_test = np.zeros([test_size, 1])\n",
    "\n",
    "index = 0\n",
    "with open('../data/quora/train.tsv') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    \n",
    "    for line in reader:\n",
    "        sent1 = line[3].translate(None, string.punctuation).lower().split()\n",
    "        sent2 = line[4].translate(None, string.punctuation).lower().split()\n",
    "#         sent1 = str.translate(line[3], str.maketrans('', '', string.punctuation)).lower().split()\n",
    "#         sent2 = str.translate(line[4], str.maketrans('', '', string.punctuation)).lower().split()            \n",
    "        dist = cosine_distance(sent1, sent2, word_vectors)\n",
    "#         dist = manhattan_distance(sent1, sent2, word_vectors)\n",
    "        \n",
    "        X_train[index] = dist\n",
    "        Y_train[index] = int(line[5])\n",
    "        \n",
    "        if index % 10000 == 0:\n",
    "            print(index)\n",
    "            \n",
    "        index += 1\n",
    "\n",
    "index = 0\n",
    "with open('../data/quora/test.tsv') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    \n",
    "    for line in reader:\n",
    "        sent1 = line[3].translate(None, string.punctuation).lower().split()\n",
    "        sent2 = line[4].translate(None, string.punctuation).lower().split()\n",
    "#         sent1 = str.translate(line[3], str.maketrans('', '', string.punctuation)).lower().split()\n",
    "#         sent2 = str.translate(line[4], str.maketrans('', '', string.punctuation)).lower().split()            \n",
    "        dist = cosine_distance(sent1, sent2, word_vectors)\n",
    "#         dist = manhattan_distance(sent1, sent2, word_vectors)\n",
    "        \n",
    "        X_test[index] = dist\n",
    "        Y_test[index] = int(line[5])\n",
    "        \n",
    "        if index % 10000 == 0:\n",
    "            print(index)\n",
    "        \n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = [0.0 if x < 0.5 else 1.0 for x in regr.predict(X_test)]\n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "num_wrong = 0\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == Y_test[i]:\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_wrong += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.89637690119946"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = float(num_correct) / (num_correct + num_wrong)\n",
    "acc * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_preds = np.zeros(len(Y_test))\n",
    "zero_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_correct_z = 0\n",
    "num_wrong_z = 0\n",
    "for i in range(len(preds)):\n",
    "    if zero_preds[i] == Y_test[i]:\n",
    "        num_correct_z += 1\n",
    "    else:\n",
    "        num_wrong_z += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6402126870285644"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(num_correct_z) / (num_correct_z + num_wrong_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
